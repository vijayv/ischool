{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier \n",
    "from sklearn import cross_validation, grid_search\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep Prediction Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I was trying to find a graceful way to easily handle all the different submission files that everyone was sending me because I need to combine them all into master file. The code below essentially just opens each file that each person sends me and adds a column to their submission with their intials + unique index num. I later pivot on this column so that each prediction will be in its own column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "team = [\"vv\", \"ar\", \"sj\", \"ns\", \"js\"]\n",
    "# Training\n",
    "for m in team:\n",
    "    path = \"ensemble/training/%s_*.csv\" % m\n",
    "    for i, f in enumerate(glob.glob(path)):\n",
    "        df = pd.read_csv(f, header=0)\n",
    "\n",
    "        if \"member\" in df.columns:\n",
    "            df.drop('member', axis=1, inplace=True)\n",
    "            \n",
    "        df[\"member\"] = m+str(i)\n",
    "        df.to_csv(f, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Testing\n",
    "for m in team:\n",
    "    path = \"ensemble/testing/%s_*.csv\" % m\n",
    "    for i, f in enumerate(glob.glob(path)):\n",
    "        df = pd.read_csv(f, header=0)\n",
    "\n",
    "        if \"member\" in df.columns:\n",
    "            df.drop('member', axis=1, inplace=True)\n",
    "            \n",
    "        df[\"member\"] = m+str(i)\n",
    "        df.to_csv(f, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Bagging Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take majority vote with equal weight to each submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ensemble = pd.DataFrame()\n",
    "for path in glob.glob('ensemble/testing/*.csv'):\n",
    "    df = pd.read_csv(path)\n",
    "    ensemble = ensemble.append(df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "passenger_id    548366\n",
      "survived           198\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "prediction = ensemble.pivot(\"passenger_id\", \"member\", \"survived\")\n",
    "prediction = prediction.mode(axis=1, numeric_only=True)\n",
    "prediction = prediction[0].reset_index()\n",
    "prediction.columns = [\"passenger_id\", \"survived\"]\n",
    "print prediction.sum()\n",
    "prediction.astype(int).to_csv(\"results/bagging_ensemble_1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fancy Ensembling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ensemble_X = pd.DataFrame()\n",
    "for path in glob.glob('ensemble/training/*.csv'):\n",
    "    df = pd.read_csv(path)\n",
    "    ensemble_X = ensemble_X.append(df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "member\n",
       "ar0       254\n",
       "js0       244\n",
       "ns0       245\n",
       "sj0       252\n",
       "vv0       256\n",
       "dtype: float64"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_features = ensemble_X.pivot(\"passenger_id\", \"member\", \"survived\")\n",
    "training_data = pd.read_csv(\"train.csv\")\n",
    "Y = training_data[\"survived\"]\n",
    "X_features.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(785,)\n",
      "RandomForestClassifier(bootstrap=True, compute_importances=None,\n",
      "            criterion='gini', max_depth=None, max_features='sqrt',\n",
      "            max_leaf_nodes=None, min_density=None, min_samples_leaf=1,\n",
      "            min_samples_split=2, n_estimators=500, n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0)\n"
     ]
    }
   ],
   "source": [
    "forest = RandomForestClassifier()\n",
    "tuned_parameters = [{'max_features': ['sqrt', 'log2'], 'n_estimators': [100, 200, 500, 1000]}]\n",
    "rf = grid_search.GridSearchCV(forest, tuned_parameters, cv=5).fit(X_features, Y)\n",
    "\n",
    "test_predictions = rf.predict(X_features)\n",
    "\n",
    "print test_predictions.shape\n",
    "print rf.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict on the Testing Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "member\n",
       "ar0       190\n",
       "js0       185\n",
       "ns0       169\n",
       "sj0       209\n",
       "vv0       207\n",
       "dtype: int64"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensemble_testing_X = pd.DataFrame()\n",
    "for path in glob.glob('ensemble/testing/*.csv'):\n",
    "    df = pd.read_csv(path)\n",
    "    ensemble_testing_X = ensemble_testing_X.append(df, ignore_index=True)\n",
    "\n",
    "X_testing_features = ensemble_testing_X.pivot(\"passenger_id\", \"member\", \"survived\")\n",
    "X_testing_features.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Take the same decision trees and run it on the test data\n",
    "test_predictions = rf.predict(X_testing_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "passenger_id    548366\n",
      "survived           175\n",
      "dtype: int64\n",
      "524\n"
     ]
    }
   ],
   "source": [
    "testing_data = pd.read_csv(\"test.csv\")\n",
    "final_predictions = zip(testing_data[\"passenger_id\"],test_predictions.astype(int))\n",
    "output_columns = \"passenger_id\", \"survived\"\n",
    "final_predictions = pd.DataFrame(final_predictions, columns=output_columns)\n",
    "print final_predictions.sum()\n",
    "print len(final_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ### Remember to change the file name with each iteration or risk losing a good submission!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "final_predictions.to_csv(\"results/2_ensemble_random_forest.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = AdaBoostClassifier(n_estimators=1000)\n",
    "clf = clf.fit(X_features, Y)\n",
    "test_predictions = clf.predict(X_testing_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "passenger_id    548366\n",
      "survived           172\n",
      "dtype: int64\n",
      "524\n"
     ]
    }
   ],
   "source": [
    "testing_data = pd.read_csv(\"test.csv\")\n",
    "final_predictions = zip(testing_data[\"passenger_id\"],test_predictions.astype(int))\n",
    "output_columns = \"passenger_id\", \"survived\"\n",
    "final_predictions = pd.DataFrame(final_predictions, columns=output_columns)\n",
    "print final_predictions.sum()\n",
    "print len(final_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ### Remember to change the file name with each iteration or risk losing a good submission!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "final_predictions.to_csv(\"results/2_ensemble_adaboost.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
